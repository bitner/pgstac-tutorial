{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6109166-5398-4b4a-9ac6-99b2e53c783f",
   "metadata": {},
   "source": [
    "# PgSTAC Tutorial\n",
    "This tutorial is designed to run in the docker compose environment defined in the docker-compose.yml file at the root of the pgstac git repository. For instructions on installing Docker and Docker Compose, you can go to https://docs.docker.com/get-docker/. The instructions provided here use the newer Compose V2 `docker compose` rather than `docker-compose` now that Docker Compose is included as part of latest version of Docker.\n",
    "\n",
    "To get started with this tutorial you should already have checked out the pgstac repository from github using `git clone https://github.com/bitner/pgstac-tutorial\n",
    "\n",
    "To start the database, a STAC FastAPI instance, and the Jupyter Notebook for this tutorial: \n",
    "```\n",
    "docker compose up -d tutorial\n",
    "```\n",
    "\n",
    "- You can then start up the workshop by going to http://localhost:8891\n",
    "- You can get to a STAC FastAPI PgSTAC instance at http://localhost:8890 (although this will not work until we have installed PgSTAC onto our Database later)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471aeb2b-806f-4ffb-ae68-3f6edb4a4a8a",
   "metadata": {},
   "source": [
    "## Tutorial Setup\n",
    "### Install magic command to use psql\n",
    "This installs a \"magic\" command for the Jupyter Notebook that will allow us to run psql commands to help us explore the database.\n",
    "The `%psql` line magic is the same as running a command using psql in the terminal.\n",
    "The `%%psql` cell magic runs the rest of the cell as the stdin to psql. This tutorial uses a system call to the psql utility. This is already installed on the Docker image that comes with this Tutorial. On Debian/Ubuntu clients, this can be installed using `sudo apt install Postgres-client`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba677f8f-90b1-4151-95ac-9b006c27be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "from IPython.display import display, HTML, JSON\n",
    "import orjson\n",
    "import shlex\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "@register_line_cell_magic\n",
    "def psql(line=None, cell=None):\n",
    "    args = [\"-X\"] + (shlex.split(line) or [\"-H\"])\n",
    "    if '-1' in args:\n",
    "        args += ['-v', 'ON_ERROR_STOP=1']\n",
    "    if cell:\n",
    "        args += ['-f', '/dev/stdin']\n",
    "    else:\n",
    "        cell = ''\n",
    "    r=Popen(['psql', *args], stdin=PIPE, stdout=PIPE, stderr=PIPE, text=True)\n",
    "    out, err = r.communicate(input=cell)\n",
    "\n",
    "    if \"-H\" in args:\n",
    "        print(err)\n",
    "        display(HTML(out))\n",
    "    elif \"-At\":\n",
    "        print(err)\n",
    "        for line in out.strip().split('\\n'):\n",
    "            try:\n",
    "                display(orjson.loads(line))\n",
    "            except:\n",
    "                display(line)\n",
    "    else:\n",
    "        print(err)\n",
    "        print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96c2b8-567a-447d-b7e2-916babbea329",
   "metadata": {},
   "source": [
    "## Check the standard Postgres environment variables \n",
    "Most tools that work with Postgres use the standard environment variables that are used by all of the tools that come standard as part of Postgres. The pypgstac python utility that comes with pgstac and is installable from pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba6284d-b570-4b01-a3ef-4b9535038441",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PGDATABASE=pgstac\n",
    "%env PGUSER=adminrole\n",
    "%env PGPASSWORD=password\n",
    "%env PGHOST=pgstac\n",
    "%env PGPORT=5432\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612d0c8b-b5b3-4676-ac59-7fa198414a59",
   "metadata": {},
   "source": [
    "Check that we can login to the database. We will use a call out to the command line psql utility using the `-l` option to list all databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b512a696-03d2-4f1d-a5b2-dd694fb853d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%psql -H -l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e76380-9548-47c5-90fa-324b352b6a43",
   "metadata": {},
   "source": [
    "## Install PgSTAC to the Database\n",
    "This docker-compose.yml with this tutorial uses the postgis/postgis:15-3.4 docker image as the base Postgres. PgSTAC requires Postgres>=14 and PostGIS>=3.1. We will now install PgSTAC on the database using the command line `pypgstac migrate` tool. Pypgstac is already installed in the docker image running this notebook. To install otherwise `pip install --upgrade pypgstac[psycopg]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60447c3a-aa07-4624-8152-46772e8afd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pypgstac migrate --debug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3567b0-15b7-43ad-9531-9ee316059737",
   "metadata": {},
   "source": [
    "## Check PgSTAC Install\n",
    "We can use the psql command line utility to login to our database now and to look around. Let's show what schemas are in the database. SQL Commands beginning with `\\` are meta commands in psql. In this case `\\dn` is asking to show all schemas (also called namespaces which explains the \"n\"). We can see that we have a \"public\" schema which is there by default in all Postgres instances as well as a \"pgstac\" schema that is owned by the \"pgstac_admin\" role - this schema as well as the pgstac_admin role were created by the `pypgstac migrate` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab81b25-da35-47b6-bccb-e2b9db64bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql\n",
    "\\dn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38217158-9051-4e4e-9777-72d4b6d7de9a",
   "metadata": {},
   "source": [
    "Postgres, much like your shell or python, has a configurable path that it uses to find anything in the database. It is controlled by the \"SEARCH_PATH\" [setting](https://www.Postgres.org/docs/current/config-setting.html) in Postgres. Postgres will search each schema in the order defined by the \"SEARCH_PATH\" to find database objects (tables, functions, views, etc). By default, the \"SEARCH_PATH\" is set to search a schema with the same name as the currently logged in role (which is \"pgstac\" with the docker environment we are using) followed by the \"public\" schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0612d-9680-4ab0-8e73-93f85fe3f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql\n",
    "SHOW SEARCH_PATH;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8ec4b-d984-4b1b-8cf9-2ad58244bf5d",
   "metadata": {},
   "source": [
    "Since we have installed PgSTAC into the \"pgstac\" schema, we need to make sure that \"pgstac\" is available in our envrionment. We can do this temporarily using the \"SET\" command in Postgres `SET SEARCH_PATH TO pgstac, public;`. Or, we can modify the setting at the DATBASE or ROLE level. For this tutorial, we will set the default setting for \"SEARCH_PATH\" at the DATABASE level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956da7f8-291b-47fa-94b5-a55ffae81811",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql\n",
    "ALTER DATABASE pgstac SET SEARCH_PATH TO pgstac,public;\n",
    "SHOW SEARCH_PATH;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef7dd6-8f23-45c9-80e8-646773aa0861",
   "metadata": {},
   "source": [
    "## Configuring Postgres Specific Settings\n",
    "There are many other settings that can be set at the SYSTEM, DATABASE, ROLE, or SESSION level. If something is set at multiple levels, the most specific level would win, so even though we have set the SEARCH_PATH at the DATABASE level, we could override it in a SESSION by using `SET SEARCH_PATH TO ...`.\n",
    "\n",
    "Out of the box as well as on most hosted services, the default Postgres configuration is extremely conservative and should be adjusted. There is *NO* one-size-fits-all set of settings even for a given database host instance size. PgSTAC comes with a function that can help to determine what a good starting point may be for some of the most important settings. Fine tuning a database can be an entire career though, so it is important to undestand some of the factors where you may want to adjust these settings. The function takes a single argument which is the memory size of the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96afe3ea-ed7a-4906-af8d-de757d4f6fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql\n",
    "SELECT check_pgstac_settings('16GB');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4aabdd-51c2-4200-86ff-4b2eaf32a1b0",
   "metadata": {},
   "source": [
    "### Important Settings That You Should Always Review\n",
    "\n",
    "#### [effective_cache_size](http://www.Postgres.org/docs/current/static/runtime-config-resource.html#GUC-SHARED-BUFFERS)\n",
    "This is the amount of memory that is estimated to be left on the system for the OS and all other processes. This is generally 1/2 to 3/4 of the total system memory.\n",
    "\n",
    "#### [shared_buffers](http://www.Postgres.org/docs/current/static/runtime-config-resource.html#GUC-SHARED-BUFFERS)\n",
    "This setting is used to tell the database how much memory is available to dedicate to Postgres for caching data. General rule-of-thumb is to set this to 1/4 of the total system memory.\n",
    "\n",
    "#### [work_mem](http://www.Postgres.org/docs/current/static/runtime-config-resource.html#GUC-WORK-MEM) and [max_connections](http://www.Postgres.org/docs/current/static/runtime-config-connection.html#GUC-MAX-CONNECTIONS)\n",
    "This is the memory that is allowed to be used per sort operation per connection for things like sorting and complex queries. This setting will really vary with the use of the database and the number of max connections that are needed in the database. In general `max_connections * work_mem` should be less than the setting for `shared_buffers`. If you have individual queries that you know will be doing larger sorts, the `work_mem` setting can be set at run time: `SET work_mem TO '40MB';`\n",
    "\n",
    "#### [maintenance_work_mem](http://www.Postgres.org/docs/current/static/runtime-config-resource.html#GUC-MAINTENANCE-WORK-MEM)\n",
    "This is the amount of memory that is made available for operations such as Vacuuming the database and Creating Indexes. This memory will only be used once at a given time, so it is OK to set this significantly higher than work_mem. 1/4 of the `shared_buffers` is a reasonable place to set this.\n",
    "\n",
    "#### [seq_page_cost](http://www.Postgres.org/docs/current/static/runtime-config-query.html#GUC-SEQUENTIAL-PAGE-COST) and [random_page_cost](https://www.Postgres.org/docs/current/runtime-config-query.html#GUC-RANDOM-PAGE-COST)\n",
    "These two variables are interpreted together and it is the ratio of `random_page_cost / seq_page_cost` that really matters. Generally, `seq_page_cost` should be left at the default of 1 and random_page_cost should be changed to reflect the nature of the underlying storage. The default for `random_page_cost` is set to 4 which is appropriate for Spinning Hard Disk Drives. For use with Solid State Drives (which is almost always what a modern hosted platform such as RDS uses), this should be set to 1.1. Having `random_page_cost` set too high can lead to wayyyy slower queries as the Postgres query planner will tend to prefer sequential scans over index scans for many queries.\n",
    "\n",
    "#### [temp_buffers](https://www.Postgres.org/docs/current/runtime-config-resource.html#GUC_TEMP_BUFFERS)\n",
    "If using Temporary Tables, increasing this setting can help to avoid spilling to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0e696-1d62-4568-823c-82eb6dae8745",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql\n",
    "ALTER SYSTEM SET EFFECTIVE_CACHE_SIZE TO '12GB';\n",
    "ALTER SYSTEM SET SHARED_BUFFERS TO '4GB';\n",
    "ALTER SYSTEM SET WORK_MEM TO '128MB';\n",
    "ALTER SYSTEM SET MAINTENANCE_WORK_MEM TO '512MB';\n",
    "ALTER SYSTEM SET MAX_CONNECTIONS TO 20;\n",
    "ALTER SYSTEM SET RANDOM_PAGE_COST TO 1.1;\n",
    "ALTER SYSTEM SET TEMP_BUFFERS TO '512MB';\n",
    "SELECT pg_reload_conf();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173af9a5-4ab2-4d8c-9b46-10ca9dcc155e",
   "metadata": {},
   "source": [
    "Note that these settings could also be set in the postgres.conf settings file on the database server or using the configuration editing tools provided by most Database as a Service providers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29b2f22-e232-480f-8a98-523fc7dd7810",
   "metadata": {},
   "source": [
    "## PgSTAC Roles\n",
    "PgSTAC installs three roles with different limitations.\n",
    "- \"pgstac_admin\" is the owner of the pgstac schema and all items in the schema. This role has the ability to use or modify anything in pgstac and should be used sparingly.\n",
    "- \"pgstac_ingest\" has read/write permissions to create items and collections in PgSTAC, but not to modify any of the PgSTAC utilities. This role should be used when you need access to ingest or modify data in the PgSTAC Catalog.\n",
    "- \"pgstac_read\" is the primary role that should be used when accessing PgSTAC when not writing any data. It is not, however, a strictly read-only role as there are still cache and statistics tables which the role will write to behind the scenes.\n",
    "\n",
    "### Assuming a role\n",
    "The role that we are using so far is an administrative or root user of the database. While you need to use a role with sufficient priviliges to create a schema, you should never use this role when accessing Postgres for working with PgSTAC. The PgSTAC roles are not set up by default to be able to login to the database, but we can use Role Inheritance to be able to assign another role with all the privileges of one of hte PgSTAC roles.\n",
    "\n",
    "Best practice would be to create a role that is used for ingest or transactional tasks and one that is used when just reading STAC Items and Collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3693b99-b612-4a1b-987b-f471964d49ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql -U adminrole\n",
    "CREATE ROLE stacrw WITH LOGIN PASSWORD 'password' IN ROLE pgstac_ingest;\n",
    "CREATE ROLE stacr WITH LOGIN PASSWORD 'password' IN ROLE pgstac_read;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b572bed8-6e2a-4cab-98df-d81ab4f34038",
   "metadata": {},
   "source": [
    "## PgSTAC Data Layout\n",
    "PgSTAC does not directly store STAC Items and Collections as JSON. Rather it pulls some of the information out into properly typed separate columns that can more effectively be used for searching through STAC Items. This data layout is intended to be a back-end implementation and particularly for the \"items\" table these tables should not be used directly for SELECT/INSERT/DELETE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81d500-3fcb-44e4-8830-355d82168d8f",
   "metadata": {},
   "source": [
    "### Items Table\n",
    "If we look a little closer at the \"items\" table, we can see that it is actually a parent partition. No data is actually stored directly in the items table, but rather in children partitions that are created through the use of triggers on the \"collections\" table. Right now, you can see that we have a foreign key constraint on the \"collections\" table (so, you must have a collection added to PgSTAC before adding any \"items\"). As of now, there are no partitions as we have not added any data yet.\n",
    "\n",
    "#### Items Table Layout\n",
    "- id: This is the id from the original JSON Item\n",
    "- geometry: The geojson from the original JSON item has been extracted and saved as a PostGIS Geometry column.\n",
    "- collection: The Collection id which is set as a Foreign Key Constraint\n",
    "- datetime: If the Item JSON has properties.datetime set, this comes from that, otherwise it comes from properties.start_datetime\n",
    "- end_datetime: If the Item JSON has properties.datetime set, this comes from that, otherwise it comes from properties.end_datetime\n",
    "- content: This is the remainder of the original JSON Item after removing the geometry, id, and collection as well as well as using a form of compression based on the common item_assets stored with a collection. This is discussed further under \"Hydration\".\n",
    "- private: This field is currently not used directly by PgSTAC, but it is to provide a place where additional private metadata about an item that is not part of the public STAC record (ie access constraints, etc) could be stored.\n",
    "\n",
    "Note that we always have a date range that we can use between datetime and end_datetime (where in the case of a \"datetime\" in the original JSON represents an instant in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02afa5b9-2c85-49dd-8ae5-4133dbc57fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql\n",
    "\\d+ items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b1ae10-de08-4e6f-8235-ad85c3422b5e",
   "metadata": {},
   "source": [
    "### Collections Table\n",
    "You'll notice that the \"collections\" table is layed out fairly similary to the Items table. These common columns help enable using the same tooling for search for both Items and Collections. As the \"collections\" table is generally much much smaller than the \"items\" table, there are fewer concerns for scalability and this table is not partitioned as is the \"items\" table.\n",
    "\n",
    "#### Collections Table Layout\n",
    "- key: This is an integer primary key that is generated upon creation\n",
    "- id: This is the id from the original JSON Collection\n",
    "- geometry: The total bounds from the Collection extent.spatial_extent has been extracted and saved as a PostGIS Geometry column.\n",
    "- datetime: The start of the Collection extent.temporal_extent\n",
    "- end_datetime: The end of the Collection extent.temporal_extent\n",
    "- content: The full original Collection JSON\n",
    "- base_item: This is used internally for \"Hydration\" process used to help compress Item records\n",
    "- private: This field is currently not used directly by PgSTAC, but it is to provide a place where additional private metadata about an item that is not part of the public STAC record (ie access constraints, etc) could be stored.\n",
    "- partition_trunc: This is used to control how finely partitioned the Items for a Collection are in the \"items\" table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5aa1f6-8fee-41ee-abe9-d32cf04eacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql\n",
    "\\d collections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4fd282",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "A number of things must happen when we load data into PgSTAC\n",
    "- All data must be transformed into the table layout for Items and Collections\n",
    "- An Items Collection must be loaded prior to any Items for that Collection\n",
    " - When adding a collection, it should be configured for the appropriate partitioning strategy.\n",
    "- For Items, PgSTAC must create any partitions that are needed prior to actually loading the data in place.\n",
    "- With partitioned data, Postgres allows for the creation of constraints that allow the Postgres query planner to skip partitions based on query predicates. When loading data, if new data must be checked and the constraints must be modified if necessary.\n",
    "\n",
    "The management of partitions and constraints can take out fairly aggressive locks on the database when done within the same transaction as loading large amounts of data. This adds a lot more overhead to any transaction loading data into the database than normal Postgres or database loads. When planning how to load data into PgSTAC, it can be very beneficial to try to group inserts into larger chunks of data (but not too large, ~10,000 seems to be a happy medium). PgSTAC should be able to reasonably handle concurrent data loads, but as the locks taken out when modifying partitions/constraints are generally at the partition level, it is good to try to avoid concurrent writes of data into the same partitions.\n",
    "\n",
    "### Pypgstac Loader\n",
    "To help mitigate some of the issues with data loading, pypgstac comes with a tool to help bulk load data into PgSTAC. When creating pipelines or doing bulk loading of data into PgSTAC, this should be the preferred method. Data can be loaded via the STAC API when exposed by STAC FastAPI, but when loading via an exposed API, the entire load is done as a single transaction and it also requires a double network hop for all the data (Client -> FastAPI -> PgSTAC)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3d2f5",
   "metadata": {},
   "source": [
    "#### Load Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1cf939",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pypgstac load collections collections.ndjson.gz --debug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157d2340-28ca-42c1-9747-5b6eb7a9d5e6",
   "metadata": {},
   "source": [
    "Now if we try to run this again, you'll notice that the insert will fail because the Collection already exists. By default, the loader will not do any conflict management when loading data and so will fail if you try to load a Collection or an Item that already exists. There is an option for pypgstac that allows us to either ignore any duplicates, or to update any existing Collection/Item with the new record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca5a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pypgstac load collections collections.ndjson.gz --debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1b00dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pypgstac load collections collections.ndjson.gz --debug --method ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ca9dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pypgstac load collections collections.ndjson.gz --debug --method upsert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6099c6cf",
   "metadata": {},
   "source": [
    "#### Load Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2184e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pypgstac load items items.ndjson.gz --debug --method ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3247858c",
   "metadata": {},
   "source": [
    "#### Partition Strategy\n",
    "By default, all Items are partitioned by Collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91291352",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql\n",
    "\\d+ items\n",
    "SELECT * FROM partitions WHERE collection='modis-13Q1-061';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de94cb",
   "metadata": {},
   "source": [
    "This now shows that there is a partition \"_items_1\" in the \"items\" table that contains all data for the 'modis-13Q1-061' data. PgSTAC can also be configured per Collection to further sub-partition data using the datetime by year or month. This is controlled by the \"partition_trunc\" column on the Collections table. There is a trigger on the Collections table that will make sure that all data is repartitioned on any change of that column. It should be noted, that it is FAR BETTER to modify the partition_trunc value before loading any data. Any time we change that value it requires a full lock on that Collection and a full rewrite of all that data.\n",
    "\n",
    "Sadly, while pulling together this workshop, I just found a bug that effects is affecting the trigger that updates the partition to the new partition strategy, there will be a fix for the bug, but won't be able to get it out before the workshop :-).\n",
    "\n",
    "For now, we will remove data from our collection, modify the partition strategy and reload our data into the new strategy.\n",
    "\n",
    "I will note one of the advantages of having partitions is that data management activities like dropping or truncating a table are much faster using partitions that selecting through all of the data to get a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91b0dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql\n",
    "TRUNCATE _items_1;\n",
    "UPDATE collections SET partition_trunc='month';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e7ac6-f293-46d0-a356-223739f5ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pypgstac load items items.ndjson.gz --debug --method=ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c728557-2ccb-41c4-8c3b-a90bbd9f148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql\n",
    "SELECT * FROM partitions;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c3550c-4e54-41e8-8bce-c025352dec47",
   "metadata": {},
   "source": [
    "We can also add a new Item using pypgstac as a library taking an iterator of Items..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc7463-ea4f-48b4-a82f-db42c6bc1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newitem1={\n",
    "    \"type\": \"Feature\",\n",
    "    \"id\":\"mynewitem1\", \n",
    "    \"collection\":\"modis-13Q1-061\", \n",
    "    \"geometry\": {\n",
    "        \"type\": \"Point\",\n",
    "        \"coordinates\": [125.6, 10.1]\n",
    "    },\n",
    "    \"properties\":{\"datetime\":\"2020-01-01 00:00:00+00\"}\n",
    "}\n",
    "newitem2={\n",
    "    \"type\": \"Feature\",\n",
    "    \"id\":\"mynewitem2\", \n",
    "    \"collection\":\"modis-13Q1-061\", \n",
    "    \"geometry\": {\n",
    "        \"type\": \"Point\",\n",
    "        \"coordinates\": [125.6, 10.1]\n",
    "    },\n",
    "    \"properties\":{\"datetime\":\"2020-02-01 00:00:00+00\"}\n",
    "}\n",
    "\n",
    "from pypgstac.db import PgstacDB\n",
    "from pypgstac.load import Loader\n",
    "with PgstacDB(debug=True) as db:\n",
    "    loader = Loader(db)\n",
    "    loader.load_items([newitem1,newitem2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef8847-0fbc-479a-bb02-cd165bb8d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql\n",
    "SELECT * FROM partitions;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2513875c-dda4-4934-b6cd-23c3530813b6",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "The Search function is the heart of PgSTAC. The Search Function converts a json stac search input (following the POST specification for a search endpoint) into the SQL needed to search through PgSTAC items. It then converts the internal storage format of the STAC Items back into the Item formatted JSON.\n",
    "\n",
    "By default the Search function will use a limit of 10 and will sort Items by datetime and id in descending order. Search also makes sure to return information that can be used for keyset pagination which can be much faster than traditional limit/offset pagination.\n",
    "\n",
    "You can control seeing the debugging information by setting the CLIENT_MIN_MESSAGES setting in Postgres to NOTICE. `SET CLIENT_MIN_MESSAGES TO NOTICE`.\n",
    "\n",
    "### Context Control\n",
    "The magic trick that PgSTAC has up it's sleeve can be seen in the debugging output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8546bb84-ef81-4216-8e38-a5f96f68355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql\n",
    "SELECT search();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95364750-878e-4e5e-af38-a8d246233609",
   "metadata": {},
   "source": [
    "You can see that the query was broken up into two chunks. From 2023-10-01 to 2023-11-01 and then from 2023-09-01 to 2023-10-01. Since we are sorting by datetime descending, we know that we can search the data in small segments and only continue if there are more rows to fetch. If we switch the sort to datetime ascending, we see that we end up running even more smaller queries. And instead we start scanning in chunks from the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598d583-cb55-4e89-b038-8a61a457215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql\n",
    "SELECT search('{\"sortby\":{\"field\":\"datetime\",\"direction\":\"asc\"}}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ae27b-c375-4304-ad2a-d0cbcc3fec05",
   "metadata": {},
   "source": [
    "Where this really comes into play is when we may have complicated queries, CPU intensive queries such as spatial lookups, or queries with properties that we haven't indexed. As long as we are sorting by datetime, we can break the problem up into smaller chunks and still be performant as we can stop looking through the data as soon as we have received the \"limit\" of rows that we have requested.\n",
    "\n",
    "This means that PgSTAC can be incredibly fast for fetching subsets of data that are sorted by time. Ordinarily if we were to run a SQL query like `SELECT * FROM mytable WHERE st_intersects(mygeomcolumn,<geometry>) ORDER BY mycolumn DESC;`, even if we have an index on both mygeomcolumn and mycolumn, Postgres is going to need to decide whether to use the spatial index to subset the data and then manually sort all the records, or it is going to use the mycolumn index to scan through the data in order and run potentially many expensive spatial operations.\n",
    "\n",
    "In the STAC specification, this is made even tougher if we chose to use the Context Extension which displays the total number of rows returned by a filter along with the number of rows that are returned after applying the limit. As you can see, the chunked approach that PgSTAC takes makes returning a smaller subset very fast, but for calculating that total count, it may need to scan through a ton of data to get the results.\n",
    "\n",
    "#### Context Control Settings\n",
    "There are a couple of settings that can be used to help mitigate the limitations of PgSTAC with providing context results.\n",
    "- context (on, off) controls whether we should calculate the total count at all\n",
    "- context_estimated_count (integer) if the estimate produced by the Postgres query planner (the results you get if you were to run EXPLAIN on a SQL query) are smaller than this number, then calculate the actual total count, otherwise just return the estimate\n",
    "- context_estimated_cost (integer) if the cost as reported by the Postgres query planner is higher than this number, return the estimate\n",
    "- context_stats_ttl (interval) Cache the results of the context calculation for this amount of time. This allows not only repeated identical searches to reuse the total context, but is especially important as paging through data as the total amount of rows is the same regardless of any changes in sorting or paging.\n",
    "\n",
    "For very large PgSTAC instances, it is recommended to turn context calculation off or to rely on the ability to use estimated counts. Do note that while for simple queries, the estimates can be quite good, as queries get more complicated, they can occasionally be wildly off.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b566e379-2efc-4955-b3fa-e775bbd4e7eb",
   "metadata": {},
   "source": [
    "## Indexing \n",
    "\n",
    "Now that we have data in our PgSTAC database, we want to make sure that we can query that data quickly. For that to work, we need to have indexes on the key properties that are used most often in our database. PgSTAC provides a mechanism to make sure that we can build and maintain indexes that will work with the way that PgSTAC creates SQL queries from the CQL2 filters that are passed to the Search function.\n",
    "\n",
    "Creating Indexes in Postgres is a double edged sword. An appropriate Index that is \"hot\" in memory can speed up a query by several orders of magnitude. But if there are too many indexes or the indexes are too large, they can have some pretty severe side effects.\n",
    "-  increasing the amount of time required for inserts/updates to data\n",
    "-  increasing the amount of time required to plan a query and to choose which indexes to use and how to use them\n",
    "-  increasing the amount of on disk data storage\n",
    "-  increasing the amount of contention for staying \"hot\" in memory\n",
    "\n",
    "When it comes to indexes, it is difficult to fight the urge to index everything because when you try a new index that fits an exact query once the difference in speed compared to not having the index can be drastic. When you look at the sum performance of your database, however, there may be another story. You should make sure to monitor the actual usage of your STAC instance and to try to match your indexing strategy to the most common use cases for your data.\n",
    "\n",
    "Given the focus on STAC on SpatioTemporal Data, PgSTAC does automatically include indexes on the id, collection, datetime, end_datetime, and geometry columns. \n",
    "\n",
    "### Queryables\n",
    "In addition to providing a place to store information to be returned by a STAC API queryables endpoint, PgSTAC uses the \"queryables\" table to define what properties of an Item for each Collection to build additional indexes on. Postgres does have the ability to build indexes on JSON fields, but these indexes are limited to help only in cases of equality comparisons and they can become very unwieldly with Items that have many or deeply nested properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7562aa31-47c8-48c5-89a6-546510e3efad",
   "metadata": {},
   "source": [
    "The \"queryables\" table contains the following information:\n",
    "- id - an internal id\n",
    "- name - the property name\n",
    "- collection_ids - an array of collection ids that this entry should apply to (NULL implies that this record applies to all collections)\n",
    "- definition - the json definition to be returned by the queryables endpoint\n",
    "- property_wrapper - a wrapper to cast the json property into a Postgres data type. It is possible to create additional custom wrappers.\n",
    "  - to_text (the default)\n",
    "  - to_int\n",
    "  - to_float\n",
    "  - to_timestamp\n",
    "- property_index_type - If this is set, then a Postgres Index will be built of this type. For most cases, this will either be NULL to set no index, or BTREE which is the default Postgres Index type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c303b0a-90c6-40b4-8545-f19cc621a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql\n",
    "SELECT id, name, collection_ids, definition, property_wrapper, property_index_type FROM queryables;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb24a83-91d7-4548-af75-981cc619ddaa",
   "metadata": {},
   "source": [
    "Note that in this setup, eo:cloud_cover has been set up to use the to_int wrapper even though the data values can be a float. This allows for smaller indexes and a bit faster ability to search and sort on eo:cloud_cover, but at the expense of a loss of precision in Search.\n",
    "\n",
    "Let's add a new index that will apply only to the modis Collection that we have loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50b2b49-c105-445d-961a-3ea809f50a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql\n",
    "INSERT INTO queryables (name, collection_ids, definition, property_wrapper, property_index_type)\n",
    "VALUES (\n",
    "    'modis:tile-id',\n",
    "    '{modis-13Q1-061}'::text[],\n",
    "    '{\"type\":\"string\"}',\n",
    "    'to_text',\n",
    "    'BTREE'\n",
    ");\n",
    "SELECT * FROM pgstac_indexes;\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c15bb6-6e21-4a7c-9fb1-74cafea85f90",
   "metadata": {},
   "source": [
    "### CQL2\n",
    "\n",
    "The STAC specification uses the \"CQL2\" (Common Query Lanbguage) as the standard for specifying arbitarily nested queries. CQL2 comes in two flavors CQL2-Text which is a very SQL like way of specifying a query and CQL2-JSON which is much more directly machine usable, but not as fun to write by hand. PgSTAC can only parse the JSON variant of CQL2 and also has legacy support for the older CQL-JSON as well as the \"query\" parameter from early in development of the STAC specification. For PgSTAC, the CQL-JSON and query variants should be considered deprecated and CQL2-JSON should only be used going forward.\n",
    "\n",
    "The pygeofilter python library can be used to translate CQL2-Text into CQL2-JSON and is what is used by the stac-fastapi-pgstac library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e95d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql\n",
    "\n",
    "SELECT search($$\n",
    "    {\"filter\":\n",
    "        {\n",
    "            \"op\": \"or\", \n",
    "            \"args\": [\n",
    "                {\"op\": \"gt\", \"args\":[{\"property\":\"modis:tile-id\"}, \"51035010\"]},\n",
    "                {\"op\": \"eq\", \"args\":[{\"property\":\"platform\"}, \"terra\"]}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "$$);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330356a-122a-416b-a7da-bfe4cd6e4409",
   "metadata": {},
   "source": [
    "## Geometry Search\n",
    "A common desire is to create on the fly mosaics of imagery based on arbitrary STAC requests. PgSTAC has additional functions which work to make these as efficient as possible. These are the functions that are behind TiTiler PgSTAC and allow it to create infinite possibilities in viewing data.\n",
    "\n",
    "This is all enabled by the GeometrySearch function.\n",
    "\n",
    "Rather than just using a set limit and paging through results, the GeometrySearch function allows you to use multiple \"escape hatches\" for when it returns the results of data. Because GeometrySearch is often used behind URL based API's such as Z/X/Y tile services that allow GET requests only, GeometrySearch uses a hash that is stored internally in PgSTAC's cacheing and statistics \"searches\" table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6397123a-dc5d-4026-a888-639ae410eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql\n",
    "SELECT * FROM search_query('{\"collections\":[\"modis-13Q1-061\"]}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a562dc6-b60f-4b71-9bf8-efb004ba2fe7",
   "metadata": {},
   "source": [
    "We can now use the hash provided in order to fetch data using the GeometrySearch Function along with these parameters:\n",
    "- geom (geometry) -- a PostGIS geometry that acts as the bounds you are trying to fill\n",
    "- queryhash (text) -- the hash returned from the search_query function\n",
    "- fields -- an include/exclude list of which properties to include in the results following the STAC Fields Extension\n",
    "- _scanlimit (int) -- the absolute limit for how many records to scan through that match the search filters before returning results\n",
    "- _limit (int) -- the limit on how many results to return\n",
    "- _timelimit (interval) -- the max amount of time to spend looking for results\n",
    "- exitwhenfull (boolean) -- once the stack of Item geometries from the sorted results would cover the entire geometry, return immediately regardless of the requested limit\n",
    "- skipcovered (boolean) -- if the coverage of an Item would be obscured by the Items returned before as sorted, do not return that Item in the results.\n",
    "\n",
    "This gives a number of dials to control tolerance for how long you are willing to wait for results and how many results you would like to deal with for different scenarios.\n",
    "\n",
    "There are also wrappers that allow for the direct use of GeoJSON (geojsonsearch or an XYZ (xyzsearch) schema.\n",
    "\n",
    "\n",
    "CREATE OR REPLACE FUNCTION geometrysearch(\n",
    "    IN geom geometry,\n",
    "    IN queryhash text,\n",
    "    IN fields jsonb DEFAULT NULL,\n",
    "    IN _scanlimit int DEFAULT 10000,\n",
    "    IN _limit int DEFAULT 100,\n",
    "    IN _timelimit interval DEFAULT '5 seconds'::interval,\n",
    "    IN exitwhenfull boolean DEFAULT TRUE, -- Return as soon as the passed in geometry is full covered\n",
    "    IN skipcovered boolean DEFAULT TRUE -- Skip any items that would show up completely under the previous items\n",
    ") RETURNS jsonb AS $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695614ad-03ac-4c9c-841c-bbd60969f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql\n",
    "SET CLIENT_MIN_MESSAGES TO WARNING;\n",
    "SELECT xyzsearch(\n",
    "    _x=>1,\n",
    "    _y=>1,\n",
    "    _z=>1,\n",
    "    queryhash=>'07c6a95fab72577b195af0691b5e11b9'::text,\n",
    "    fields=>'{\"include\":[\"id\"]}'::jsonb,\n",
    "    exitwhenfull=>TRUE,\n",
    "    skipcovered=>TRUE\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411311b3",
   "metadata": {},
   "source": [
    "## Hydration\n",
    "PgSTAC uses a method to compress the data stored by an Item by using the \"item_assets\" attribute on the Item's Collection. When an Item is added, a 'diff' is created between the Assets of an Item and the \"item_assets\" on the Collection. Any duplicate data in the Item content that can be derived from the Collection is then stripped from the Item content as it is stored in PgSTAC. PgSTAC refers to this process as 'dehydration'.\n",
    "\n",
    "When a search is returned from PgSTAC, this information is then merged back into the Item pior to be returned. PgSTAC refersw to this process as 'hydration'.\n",
    "\n",
    "For Collections that have very large Assets, this can save a huge amount of disk space as well as the availability of memory in the Postgres instance. This process is, however, more CPU intensive. There is an option when returning a search that allows for the return of the dehydrated data. This can be done using the '{\"conf\":{\"nohydrate\":true}}' parameter as part of the search.\n",
    "\n",
    "Pypgstac has the ability to do shift this dehydration process away from the Database shifting the load to the application server and speeding things up, particularly under heavy load when multiple application server instances are able to process the results. At PgSTAC version 0.8, this process was sped up even further in pypgstac through implenting the dehydration in Rust. The default setting in STAC FastAPI is to use application side dehydration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8dd0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql -At\n",
    "SET CLIENT_MIN_MESSAGES TO WARNING;\n",
    "select search('{\"limit\":1}'::jsonb)->'features'->0->'assets'->'hdf';\n",
    "select content->'assets'->'hdf' from items limit 1;\n",
    "select content->'item_assets'->'hdf' from collections limit 1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdc8897",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql -At\n",
    "select search()->'features'->0->'assets'->'hdf';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0649795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%psql -At\n",
    "select search('{\"conf\":{\"nohydrate\":true}}'::jsonb)->'features'->0->'assets'->'hdf';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b39d29-b81d-4379-a9db-43869d9c7fba",
   "metadata": {},
   "source": [
    "# Considerations for deployment with STAC-FastAPI\n",
    "Postgres is limited in communicating over it's own protocol and cannot serve functions data directly over http. PgSTAC has been designed to try to put as much of the heavy lifting of querying a large amount of STAC Items as close to the data itself as possible. It is designed so that building a STAC API should only require a minimal set of wrappers around calls to a PgSTAC Database. STAC-FastAPI-PgSTAC has been built in tandem with PgSTAC and allows to keep the business logic of optimizing queries over large search results close to the data while being able to offload some of the work of serving STAC records onto more easily scalable Application Server instances.\n",
    "\n",
    "## Fast API\n",
    "Being built upon Fast API, STAC FastAPI allows the integration of STAC with other related API's (See EOAPI as an example) as well as to add additional capabilities such as authentication, cacheing, or compression without needing to modify STAC-FastAPI itself.\n",
    "\n",
    "- Deploy using a role that inerhits from pgstac_ingest for the POSTGRES_WRITE_HOST and from pgstac_read for the POSTGRES_READ_HOST\n",
    "- Make sure to use the USE_API_HYDRATE=true setting\n",
    "- If deploying using Lambdas or other highly auto-scaling serverless approaches, only use a single connection for the database pool on the application server and deploy alongside a separate connection pooler such as pgbouncer\n",
    "- If you have default landing queries, make sure that you have some sort of cacheing layer on top of them\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
